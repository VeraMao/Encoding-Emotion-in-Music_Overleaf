
@incollection{Tuba2025,
	address = {Singapore},
	title = {A {Comparative} {Analysis} of {Multi}-label {Emotion} {Recognition} in {Music}},
	volume = {1159},
	isbn = {978-981-97-8525-4 978-981-97-8526-1},
	url = {https://link.springer.com/10.1007/978-981-97-8526-1_16},
	language = {en},
	urldate = {2025-05-14},
	booktitle = {{ICT} {Systems} and {Sustainability}},
	publisher = {Springer Nature Singapore},
	author = {Uplabdhee, Avni and Singh, Vaishali and Jain, Palak and Seeja, K. R.},
	editor = {Tuba, Milan and Akashe, Shyam and Joshi, Amit},
	year = {2025},
	pages = {201--213},
}

@inproceedings{Ahsan2015,
	address = {Kolkata, India},
	title = {Multi-label annotation of music},
	isbn = {978-1-4799-7458-0},
	url = {http://ieeexplore.ieee.org/document/7050685/},
	doi = {10.1109/ICAPR.2015.7050685},
	urldate = {2025-05-14},
	booktitle = {2015 {Eighth} {International} {Conference} on {Advances} in {Pattern} {Recognition} ({ICAPR})},
	publisher = {IEEE},
	author = {Ahsan, Hiba and Kumar, Vijay and Jawahar, C.V.},
	month = jan,
	year = {2015},
	pages = {1--5},
}

@article{Casey2008,
	title = {Content-{Based} {Music} {Information} {Retrieval}: {Current} {Directions} and {Future} {Challenges}},
	volume = {96},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Content-{Based} {Music} {Information} {Retrieval}},
	url = {http://ieeexplore.ieee.org/document/4472077/},
	doi = {10.1109/JPROC.2008.916370},
	number = {4},
	urldate = {2025-04-26},
	journal = {Proceedings of the IEEE},
	author = {Casey, M.A. and Veltkamp, R. and Goto, M. and Leman, M. and Rhodes, C. and Slaney, M.},
	month = apr,
	year = {2008},
	pages = {668--696},
}

@article{Acheampong2021,
	title = {Transformer models for text-based emotion detection: a review of {BERT}-based approaches},
	volume = {54},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Transformer models for text-based emotion detection},
	url = {https://link.springer.com/10.1007/s10462-021-09958-2},
	doi = {10.1007/s10462-021-09958-2},
	language = {en},
	number = {8},
	urldate = {2025-04-26},
	journal = {Artificial Intelligence Review},
	author = {Acheampong, Francisca Adoma and Nunoo-Mensah, Henry and Chen, Wenyu},
	month = dec,
	year = {2021},
	pages = {5789--5829},
}

@inproceedings{Parthasarathy2017,
	address = {New Orleans, LA},
	title = {Ranking emotional attributes with deep neural networks},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7953107/},
	doi = {10.1109/ICASSP.2017.7953107},
	abstract = {Studies have shown that ranking emotional attributes through preference learning methods has signiﬁcant advantages over conventional emotional classiﬁcation/regression frameworks. Preference learning is particularly appealing for retrieval tasks, where the goal is to identify speech conveying target emotional behaviors (e.g., positive samples with low arousal). With recent advances in deep neural networks (DNNs), this study explores whether a preference learning framework relying on deep learning can outperform conventional ranking algorithms. We use a deep learning ranker implemented with the RankNet algorithm to evaluate preference between emotional sentences in terms of dimensional attributes (arousal, valence and dominance). The results show improved performance over ranking algorithms trained with support vector machine (SVM) (i.e., RankSVM). The results are signiﬁcantly better than performance reported in previous work, demonstrating the potential of RankNet to retrieve speech with target emotional behaviors.},
	language = {en},
	urldate = {2025-04-26},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Parthasarathy, Srinivas and Lotfian, Reza and Busso, Carlos},
	month = mar,
	year = {2017},
	pages = {4995--4999},
}

@incollection{Longo2024,
	address = {Cham},
	title = {A {Comparative} {Analysis} of {SHAP}, {LIME}, {ANCHORS}, and {DICE} for {Interpreting} a {Dense} {Neural} {Network} in {Credit} {Card} {Fraud} {Detection}},
	volume = {2156},
	isbn = {978-3-031-63802-2 978-3-031-63803-9},
	url = {https://link.springer.com/10.1007/978-3-031-63803-9_20},
	language = {en},
	urldate = {2025-04-26},
	booktitle = {Explainable {Artificial} {Intelligence}},
	publisher = {Springer Nature Switzerland},
	author = {Raufi, Bujar and Finnegan, Ciaran and Longo, Luca},
	editor = {Longo, Luca and Lapuschkin, Sebastian and Seifert, Christin},
	year = {2024},
	doi = {10.1007/978-3-031-63803-9_20},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {365--383},
}

@article{Russell1982,
	title = {The structure in persons' implicit taxonomy of emotions},
	volume = {16},
	issn = {0092-6566},
	url = {https://www.sciencedirect.com/science/article/pii/0092656682900058},
	doi = {https://doi.org/10.1016/0092-6566(82)90005-8},
	abstract = {Persons' classifications of emotions are viewed as based on an implicit taxonomy of emotions. This implicit taxonomy has sometimes been thought of as a bipolar dimensional system within which are located all the various emotion-descriptive categories (fear, anger, happiness, etc.). A dimensional taxonomy of this kind assumes that all emotion categories are interrelated in a systematic way. More often, however, the implicit taxonomy has been thought of as a list of separate emotions. A taxonomy in the form of a list typically presupposes that emotion categories are either synonymous, independent, or mutually exclusive. McNair, Lorr, and Droppleman's Profile of Mood States (San Diego: Educational and Industrial Testing Service, 1971) is one such list of emotion categories and was therefore examined in two studies for the actual interrelationships among its categories. To examine intraindividual relationships, 45 subjects rated the emotional state posed in each of 32 videotape segments. Even at the level of the individual subject, results showed that emotion categories are systematically interrelated and can be accounted for reasonably well by a system of three bipolar dimensions: pleasure-displeasure, arousal-sleepiness, and dominance-submissiveness. Evidence for the same bipolar system was also obtained in a second study, which examined interindividual differences in the self-reported emotional states of 343 subjects.},
	number = {4},
	journal = {Journal of Research in Personality},
	author = {Russell, James A and Steiger, James H},
	year = {1982},
	pages = {447--469},
}

@inproceedings{Olha2023,
	title = {Method for {Sentiment} {Analysis} of {Ukrainian}-{Language} {Reviews} in {E}-{Commerce} {Using} {RoBERTa} {Neural} {Network}},
	url = {https://api.semanticscholar.org/CorpusID:258688336},
	booktitle = {International {Conference} on {Computational} {Linguistics} and {Intelligent} {Systems}},
	author = {{Olha Zalutska} and {Maryna Molchanova} and {Olena Sobko} and {Olexander Mazurets} and {Oleksandr Pasichnyk} and {Olexander V. Barmak} and {Iurii Krak}},
	year = {2023},
}

@inproceedings{Cano2017,
	title = {Music {Mood} {Dataset} {Creation} {Based} on {Last} {FM} {Tags}},
	isbn = {978-1-921987-66-3},
	url = {http://airccj.org/CSCP/vol7/csit76803.pdf},
	doi = {10.5121/csit.2017.70603},
	urldate = {2025-04-26},
	booktitle = {Computer {Science} \& {Information} {Technology} ({CS} \& {IT})},
	publisher = {Academy \& Industry Research Collaboration Center (AIRCC)},
	author = {Cano, Erion and Morisio, Maurizio},
	month = may,
	year = {2017},
	pages = {15--26},
}

@article{Kamenetsky1997,
	title = {Effect of {Tempo} and {Dynamics} on the {Perception} of {Emotion} in {Music}},
	volume = {25},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0305-7356, 1741-3087},
	url = {https://journals.sagepub.com/doi/10.1177/0305735697252005},
	doi = {10.1177/0305735697252005},
	abstract = {Adults (N = 96) with little or no training in music heard one of four possible MIDI versions of each of four musical excerpts. The four versions of each excerpt included one with unvarying tempo and dynamics, one with variations in tempo only, one with variations in dynamics only, and one with variations in tempo and dynamics. Participants rated each excerpt on a 7-point scale for likeability and emotional expressiveness. Variations in dynamics resulted in higher ratings on both measures but variations in tempo had no such effect. In general, women rated the musical excerpts as more emotionally expressive and more likeable than did men. Finally, musical preferences were highly correlated with ratings of emotional expressiveness.},
	language = {en},
	number = {2},
	urldate = {2025-04-26},
	journal = {Psychology of Music},
	author = {Kamenetsky, Stuart B. and Hill, David S. and Trehub, Sandra E.},
	month = oct,
	year = {1997},
	pages = {149--160},
}

@misc{Artemova2025,
	title = {Hands-{On} {Tutorial}: {Labeling} with {LLM} and {Human}-in-the-{Loop}},
	shorttitle = {Hands-{On} {Tutorial}},
	url = {http://arxiv.org/abs/2411.04637},
	doi = {10.48550/arXiv.2411.04637},
	abstract = {Training and deploying machine learning models relies on a large amount of human-annotated data. As human labeling becomes increasingly expensive and time-consuming, recent research has developed multiple strategies to speed up annotation and reduce costs and human workload: generating synthetic training data, active learning, and hybrid labeling. This tutorial is oriented toward practical applications: we will present the basics of each strategy, highlight their benefits and limitations, and discuss in detail real-life case studies. Additionally, we will walk through best practices for managing human annotators and controlling the quality of the final dataset. The tutorial includes a hands-on workshop, where attendees will be guided in implementing a hybrid annotation setup. This tutorial is designed for NLP practitioners from both research and industry backgrounds who are involved in or interested in optimizing data labeling projects.},
	urldate = {2025-04-21},
	publisher = {arXiv},
	author = {Artemova, Ekaterina and Tsvigun, Akim and Schlechtweg, Dominik and Fedorova, Natalia and Chernyshev, Konstantin and Tilga, Sergei and Obmoroshev, Boris},
	month = jan,
	year = {2025},
	note = {arXiv:2411.04637 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{Kim2024,
	title = {{MEGAnno}+: {A} {Human}-{LLM} {Collaborative} {Annotation} {System}},
	shorttitle = {{MEGAnno}+},
	url = {http://arxiv.org/abs/2402.18050},
	doi = {10.48550/arXiv.2402.18050},
	abstract = {Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.},
	urldate = {2025-04-21},
	publisher = {arXiv},
	author = {Kim, Hannah and Mitra, Kushan and Chen, Rafael Li and Rahman, Sajjadur and Zhang, Dan},
	month = feb,
	year = {2024},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{SpotifyAPI,
	title = {Spotify {Web} {API}},
	url = {https://developer.spotify.com/documentation/web-api},
}

@misc{GrosseMalte2022,
	title = {8+ {M}. {Spotify} {Tracks}, {Genre}, {Audio} {Features}},
	url = {https://www.kaggle.com/datasets/maltegrosse/8-m-spotify-tracks-genre-audio-features/data},
	author = {Grosse, Malte},
	year = {2022},
}

@inproceedings{MillionSongDataset,
	title = {The {Million} {Song} {Dataset}},
	url = {https://labrosa.ee.columbia.edu/millionsong},
	booktitle = {Proceedings of the 12th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR} 2011)},
	author = {Thierry, Bertin-Mahieux and Daniel, P.W. Ellis and Brian, Whitman and Paul, Lamere},
	year = {2011},
}

@misc{Hartman2022,
	title = {Emotion {English} {DistilRoBERTa}-base},
	url = {https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/},
	author = {Hartmann, Jochen},
	year = {2022},
}

@misc{Mikolov2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2025-04-11},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	keywords = {Computer Science - Computation and Language},
}

@article{Bandhakavi2017,
	title = {Lexicon {Generation} for {Emotion} {Detection} from {Text}},
	volume = {32},
	issn = {1941-1294},
	url = {https://ieeexplore.ieee.org/abstract/document/7851145?casa_token=BP2dXjPKQEMAAAAA:WQKU9husdL6_1NViXxmikxT2WNKZjLOUuIwpCERQyoEBOYrjinE9JdqY_uMh3vNO7WNOHdsIVf0},
	doi = {10.1109/MIS.2017.22},
	abstract = {General-purpose emotion lexicons (GPELs) that associate words with emotion categories remain a valuable resource for emotion detection. However, the static and formal nature of their vocabularies make them an inadequate resource for detecting emotions in domains that are inherently dynamic in nature. This calls for lexicons that are not only adaptive to the lexical variations in a domain but which also provide finer-grained quantitative estimates to accurately capture word-emotion associations. In this article, the authors demonstrate how to harness labeled emotion text (such as blogs and news headlines) and weakly labeled emotion text (such as tweets) to learn a word-emotion association lexicon by jointly modeling emotionality and neutrality of words using a generative unigram mixture model (UMM). Empirical evaluation confirms that UMM generated emotion language models (topics) have significantly lower perplexity compared to those from state-of-the-art generative models like supervised Latent Dirichlet Allocation (sLDA). Further emotion detection tasks involving word-emotion classification and document-emotion ranking confirm that the UMM lexicon significantly out performs GPELs and also state-of-the-art domain specific lexicons.},
	number = {1},
	urldate = {2025-04-06},
	journal = {IEEE Intelligent Systems},
	author = {Bandhakavi, Anil and Wiratunga, Nirmalie and Massie, Stewart and Padmanabhan, Deepak},
	month = jan,
	year = {2017},
	keywords = {Emotion recognition, Intelligent systems, Mathematical model, Mixture models, Sentiment analysis, Social network services, Vocabulary, domain-specific lexicon, emotion detection, emotion ranking, intelligent systems, mixture model, word classification},
	pages = {102--108},
}

@inproceedings{Chauhan2024,
	address = {Pimari Chinchwad, India},
	title = {Real {Time} {Mood} {Detection} on {Music} {Streaming} {Platforms}: {A} {Deep} learning perspective},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5421-8},
	shorttitle = {Real {Time} {Mood} {Detection} on {Music} {Streaming} {Platforms}},
	url = {https://ieeexplore.ieee.org/document/10837722/},
	doi = {10.1109/ASIANCON62057.2024.10837722},
	urldate = {2025-03-30},
	booktitle = {2024 4th {Asian} {Conference} on {Innovation} in {Technology} ({ASIANCON})},
	publisher = {IEEE},
	author = {Chauhan, Rahul and Vashisht, Anshul and Negi, Abhishek and Devliyal, Swati},
	month = aug,
	year = {2024},
	pages = {1--6},
}

@article{Garg2022,
	title = {Machine learning model for mapping of music mood and human emotion based on physiological signals},
	volume = {81},
	issn = {1380-7501, 1573-7721},
	url = {https://link.springer.com/10.1007/s11042-021-11650-0},
	doi = {10.1007/s11042-021-11650-0},
	language = {en},
	number = {4},
	urldate = {2025-03-30},
	journal = {Multimedia Tools and Applications},
	author = {Garg, Anupam and Chaturvedi, Vybhav and Kaur, Arman Beer and Varshney, Vedansh and Parashar, Anshu},
	month = feb,
	year = {2022},
	pages = {5137--5177},
}

@article{Rosner2018,
	title = {Automatic music genre classification based on musical instrument track separation},
	volume = {50},
	issn = {0925-9902, 1573-7675},
	url = {http://link.springer.com/10.1007/s10844-017-0464-5},
	doi = {10.1007/s10844-017-0464-5},
	language = {en},
	number = {2},
	urldate = {2025-03-30},
	journal = {Journal of Intelligent Information Systems},
	author = {Rosner, Aldona and Kostek, Bozena},
	month = apr,
	year = {2018},
	pages = {363--384},
}

@article{Perlovsky2010,
	title = {Musical emotions: {Functions}, origins, evolution},
	volume = {7},
	issn = {1571-0645},
	url = {https://www.sciencedirect.com/science/article/pii/S1571064509000438},
	doi = {https://doi.org/10.1016/j.plrev.2009.11.001},
	number = {1},
	journal = {Physics of Life Reviews},
	author = {Perlovsky, Leonid},
	year = {2010},
	keywords = {Cognitive dissonance, Culture, Emotions, Evolution, Knowledge instinct, Language, Mathematical models, Mind, Music, Neural mechanisms},
	pages = {2--27},
}

@inproceedings{Joseph2019,
	title = {Machine learning approaches for emotion classification of music: a systematic literature review},
	doi = {10.1109/ICAC49085.2019.9103378},
	booktitle = {2019 international conference on advancements in computing ({ICAC})},
	author = {Joseph, Charles and Lekamge, Sugeeswari},
	year = {2019},
	keywords = {Bibliographies, Databases, Emotion recognition, Feature extraction, Machine learning, Music, Systematics, machine learning, music emotion, music emotion classification, systematic literature review},
	pages = {334--339},
}

@article{Yoo2024,
	title = {Emotion {Recognition} and {Multi}-class {Classification} in {Music} with {MFCC} and {Machine} {Learning}},
	volume = {14},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {2460-6952, 2088-5334},
	url = {https://ijaseit.insightsociety.org/index.php/ijaseit/article/view/18671},
	doi = {10.18517/ijaseit.14.3.18671},
	number = {3},
	urldate = {2025-03-30},
	journal = {International Journal on Advanced Science, Engineering and Information Technology},
	author = {Yoo, Gilsang and Hong, Sungdae and Kim, Hyeocheol},
	month = jun,
	year = {2024},
	pages = {818--825},
}

@inproceedings{Juthi2020,
	address = {Cham},
	title = {Music emotion recognition with the extraction of audio features using machine learning approaches},
	isbn = {978-3-030-30577-2},
	booktitle = {Proceedings of {ICETIT} 2019},
	publisher = {Springer International Publishing},
	author = {Juthi, Jannatul Humayra and Gomes, Anthony and Bhuiyan, Touhid and Mahmud, Imran},
	editor = {Singh, Pradeep Kumar and Panigrahi, Bijaya Ketan and Suryadevara, Nagender Kumar and Sharma, Sudhir Kumar and Singh, Amit Prakash},
	year = {2020},
	pages = {318--329},
}

@article{Xu2011,
	title = {Using machine learning analysis to interpret the relationship between music emotion and lyric features},
	volume = {7},
	issn = {2376-5992},
	url = {https://doi.org/10.7717/peerj-cs.785},
	doi = {10.7717/peerj-cs.785},
	journal = {PeerJ Computer Science},
	author = {Xu, Liang and Sun, Zaoyi and Wen, Xin and Huang, Zhengxi and Chao, Chi-ju and Xu, Liuchang},
	month = nov,
	year = {2021},
	keywords = {Audio signal processing, Chinese pop song, LIWC, Lyric feature extraction, Music emotion recognition},
	pages = {e785},
}

@article{Xia2022,
	title = {Study on music emotion recognition based on the machine learning model clustering algorithm},
	volume = {2022},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/9256586},
	doi = {https://doi.org/10.1155/2022/9256586},
	number = {1},
	journal = {Mathematical Problems in Engineering},
	author = {Xia, Yu and Xu, Fumei},
	year = {2022},
	pages = {9256586},
}

@article{Li2023,
	title = {Emotion recognition of music based on machine learning scenarios},
	volume = {39},
	doi = {10.54097/hset.v39i.6515},
	journal = {Highlights in Science, Engineering and Technology},
	author = {Li, Zenan},
	month = apr,
	year = {2023},
	pages = {144--150},
}

@article{McCraty1998,
	title = {The effects of different types of music on mood, tension, and mental clarity},
	volume = {4},
	journal = {Alternative therapies in health and medicine},
	author = {McCraty, Rollin and Barrios-Choplin, B and Atkinson, M and Tomasino, Dana},
	month = feb,
	year = {1998},
	pages = {75--84},
}

@article{Yang2024,
	title = {Comparison and analysis of prediction accuracy between traditional machine learning algorithms and {XGBoost} algorithm in music emotion classification},
	volume = {57},
	doi = {10.54254/2755-2721/57/20241316},
	journal = {Applied and Computational Engineering},
	author = {Yang, Mengxi},
	month = apr,
	year = {2024},
	pages = {98--103},
}

@inproceedings{Helmholz2019,
	title = {Feel the moosic: {Emotion}-based music selection and recommendation},
	doi = {10.18690/978-961-286-280-0.11},
	author = {Helmholz, Patrick and Meyer, Michael and Robra-Bissantz, Susanne},
	month = jun,
	year = {2019},
}

@article{Huron2015,
	title = {Affect induction through musical sounds: an ethological perspective},
	volume = {370},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2014.0098},
	doi = {10.1098/rstb.2014.0098},
	number = {1664},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Huron, David},
	year = {2015},
	pages = {20140098},
}

@article{Leubner2017,
	title = {Reviewing the effectiveness of music interventions in treating depression},
	volume = {8},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.01109},
	doi = {10.3389/fpsyg.2017.01109},
	journal = {Frontiers in Psychology},
	author = {Leubner, Daniel and Hinterberger, Thilo},
	year = {2017},
}
